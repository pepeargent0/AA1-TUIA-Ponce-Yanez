{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a62f23f-3431-4d69-bd27-d4be202d4c49",
   "metadata": {},
   "source": [
    "## TUIA - Aprendizaje Automático 1\n",
    "\n",
    "### Trabajo Práctico: Predicción de lluvia en Australia.\n",
    "\n",
    "### Integrantes:\n",
    "- Ponce, Daniel\n",
    "- Yañez, Mirian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eabeef9-e732-4a21-bd59-f842c1524b46",
   "metadata": {},
   "source": [
    "El presente informe detalla el trabajo práctico llevado a cabo para la predicción de las condiciones climáticas en Australia, centrándonos en las ciudades de Adelaide, Canberra, Cobar, Dartmoor, Melbourne, MelbourneAirport, MountGambier, Sydney y SydneyAirport. \n",
    "\n",
    "El conjunto de datos utilizado se denomina weatherAUS.csv y contiene información climática de los últimos diez años.\n",
    "\n",
    "## Variables de Interés:\n",
    "\n",
    "**RainTomorrow y RainfallTomorrow**: Estas variables representan nuestro objetivo de predicción, indicando si lloverá al día siguiente y la cantidad de lluvia, respectivamente.\n",
    "\n",
    "\"RainTomorrow\" (categórica, para un problema de clasificación)\n",
    "\n",
    "\"RainfallTomorrow\" (continua, para un problema de regresión)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a1666-32bc-428d-8346-d2f06da55f01",
   "metadata": {},
   "source": [
    "## Paquetes y Librerias"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pandas scikit-learn seaborn imblearn\n",
   "id": "173f2bf9c27c6fa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import SGDRegressor, Ridge, Lasso, ElasticNet, LogisticRegression, ElasticNetCV, RidgeCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "import shap\n",
    "import warnings\n",
    "import optuna\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "485bc07e-6832-40f1-9d1f-8ef2ebcc32c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_metrics_regresion(y, y_pred, title, nr_neuronal=True):\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    print(title)\n",
    "    print(\"Mean Squared Error :\", mse)\n",
    "    print(\"R-squared:\", r2)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    if nr_neuronal:\n",
    "        mape = np.mean(np.abs((y - y_pred) / y)) * 100\n",
    "        print(\"Mean Absolute Percentage Error (MAPE):\", mape)\n",
    "    \n",
    "    \n"
   ],
   "id": "1daec272d64b6c84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importar el dataset",
   "id": "9dfd04e7-45dc-49f8-b05b-0ec0eaa7ffc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path= 'weatherAUS.csv'\n",
    "df = pd.read_csv(file_path, sep=',')"
   ],
   "id": "f33f6b14-a3dc-47b1-aa75-942e8aaad034",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filtramos por las ciudades de interés y convertimos a formato date la fecha\n",
   "id": "e63b9b1b-8fac-4e8d-b0c7-9636bc899b30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ciudades = ['Adelaide', 'Canberra', 'Cobar', 'Dartmoor', 'Melbourne', 'MelbourneAirport', 'MountGambier', 'Sydney', 'SydneyAirport']\n",
    "df = df[df['Location'].isin(ciudades)]\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ],
   "id": "8928b114-fd9b-47c4-8f20-0ff33e465395",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizamos la distribución de los datos por año",
   "id": "330ac8ba7c1946a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['Year'] = pd.to_datetime(df['Date']).dt.year # Creamos la columna Year para poder realizar la visualización por año\n",
    "\n",
    "# Contamos la cantidad de datos por año\n",
    "data_by_year = df['Year'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "data_by_year.plot(kind='bar', color='skyblue')\n",
    "plt.title('Cantidad de datos por año (2007-2017)')\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "df = df.drop(columns=['Year']) # Eliminamos la columna Year"
   ],
   "id": "1b474456f7203d7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Realizamos el split de entrenamiento y prueba \n",
    "\n",
    "Decidimos dividir el conjunto de datos de forma manual. Esta elección se debe a que posteriormente imputaremos los valores basados en la fecha. Si utilizáramos la función train_test_split, la separación sería aleatoria, lo que podría provocar una fuga de datos.\n"
   ],
   "id": "7ee2a3bc-596b-4d80-9ae9-b9fc350b4885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Definimos las fechas límite para la división\n",
    "date_train_limit = pd.to_datetime('2015-10-06')\n",
    "\n",
    "# Filtramos el DataFrame para obtener los conjuntos de entrenamiento y prueba\n",
    "train = df[df['Date'] <= date_train_limit]\n",
    "test = df[df['Date'] > date_train_limit]\n",
    "\n",
    "print(f\"El conjunto de entrenamiento tiene {len(train)} registros y va hasta la fecha {date_train_limit}.\")\n",
    "print(f\"El conjunto de prueba tiene {len(test)} registros y empieza a partir de la fecha {date_train_limit}.\")"
   ],
   "id": "5ba04724ac11f7e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Análisis descriptivo\n",
    "\n",
    "Se realizará un análisis exploratorio del conjunto de datos de entrenamiento para entender sus características principales y determinar si se requiere alguna acción para abordar datos faltantes, valores atípicos, la codificación de variables categóricas u otros procesos antes de proceder."
   ],
   "id": "6cb83657-20a4-4341-85e2-5f0fe6165928"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.columns",
   "id": "88ae449c-232d-48e2-9ace-ceb8ccbe5cdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.head()",
   "id": "34837b49-4c97-4d44-9d9f-268783138e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train.info()\n",
    "train.isna().sum()"
   ],
   "id": "ed9405a5-c396-41ef-a9d8-32b6fc753ede",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Hay un total de 22590 datos de entrenamiento, 25 columnas y se puede observar que hay datos nulos en la mayoria de las variables.**",
   "id": "be6006fd-be46-4814-b48a-1b45470f22c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Eliminamos la columna 'Unnamed: 0' y 'Location'\n",
    "Eliminamos la columna Location ya que vamos a considerarlas a todas como una sola"
   ],
   "id": "2d4d465d-8e27-4f2f-a2b2-488d2e4ddb83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train = train.drop(columns=['Unnamed: 0', 'Location'])",
   "id": "c3fb1ffd-9f0b-4b31-86b0-9564964a4ec6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cambiamos los valores nulos\n",
    "Debido a que tomamos las ciudades como una única localidad, decidimos reemplazar los valores faltantes por otro de la misma fecha o, en su defecto por la mas cercana."
   ],
   "id": "fcf84609-33f6-42ea-82b3-4e8f66d3b9f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train.sort_values(by='Date', inplace=True)\n",
    "train.fillna(method='ffill', inplace=True)"
   ],
   "id": "b78e18e2-cc0a-4ded-aba2-c1b8cfb1d1cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.isna().sum()",
   "id": "aa1597a0-b60a-43f9-9f05-c0b0e984c991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Eliminamos la columna Date:\n",
    "La razon es que ya no la vamos a usar, solo la usamos como criterio para el reemplazo de los valores nulos"
   ],
   "id": "42a1101d24ee1cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train = train.drop(columns=['Date'])",
   "id": "7277bfc874881b79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.columns",
   "id": "9ba2fa096762cc54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creación de columnas \n",
    "Con el objetivo de reducir la cantidad de columnas y mejorar la explicabilidad del modelo, decidimos agrupar las variables que representan dos momentos del día en una sola columna."
   ],
   "id": "fb02c9aa3a8a3bd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_aggregate = ['Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'Humidity9am', \n",
    "                        'Humidity3pm', 'Cloud9am', 'Cloud3pm','WindSpeed3pm','WindSpeed9am']\n",
    "new_columns = []\n",
    "train['PressureVariation'] = train['Pressure3pm'] - train['Pressure9am']\n",
    "train['TempVariation'] = train['Temp3pm'] - train['Temp9am']\n",
    "train['HumidityVariation'] = train['Humidity3pm'] - train['Humidity9am']\n",
    "train['CloudVariation'] = train['Cloud3pm'] - train['Cloud9am']\n",
    "train['WindSpeedVariation'] = train['WindSpeed3pm'] - train['WindSpeed9am']\n",
    "train.drop(columns=columns_to_aggregate, inplace=True)\n"
   ],
   "id": "412d02df41374c0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.columns",
   "id": "f664a4ede898e111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Estadística descriptiva de las variables numéricas\n",
    "Examinamos medidas estadísticas, incluyendo valores mínimos, máximos, cuartiles, y medidas de centralidad como la mediana y la media."
   ],
   "id": "64c5771ed05b410f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.describe()",
   "id": "2318fe48211310b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Al analizar las columnas, se percibe que tanto la media como la mediana muestran una cercanía notable, lo que sugiere una tendencia consistente en los datos. Los desvíos en la dispersión de los datos no son extremos y la distribución no presentaria una gran extensión en un boxplot.\n",
    "\n",
    "Los valores mínimos y máximos registrados estan considerablemente alejados de los valores centrales, lo cual señala la posible existencia de valores atípicos dentro del conjunto de datos.\n",
    "\n",
    "## **Bloxplot**"
   ],
   "id": "9fb05c43c2733b31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_columns = train.select_dtypes(include=[np.float64]).columns\n",
    "\n",
    "colores = sns.color_palette('husl', n_colors=len(numeric_columns))\n",
    "\n",
    "fig, axes = plt.subplots(len(numeric_columns), 1, figsize=(15, 20), sharex=False)\n",
    "\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    sns.boxplot(data=train, x=col, ax=axes[i], color=colores[i], orient='h')\n",
    "    axes[i].set_title(f'Boxplot de {col}')\n",
    "    axes[i].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "410a8863cdfa8297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Histograma",
   "id": "d2cb95b665847367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_columns = train.select_dtypes(include=[np.float64]).columns\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, column in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(4, 5, i)\n",
    "    sns.histplot(train[column].dropna(), kde=True)\n",
    "    plt.title(f'Histograma de {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6199b440b274d64a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para comprender más a fondo el comportamiento de los datos, empleamos histogramas donde se puede apreciar que algunas variables exhiben una distribución más uniforme de sus valores, como es el caso de la temperatura, humedad y presión. Por otro lado, existen variables que muestran la presencia de múltiples modas en los datos y una distribución menos uniforme.\n",
    "\n",
    "Asimismo, se destaca que la variable Rainfall concentra la gran mayoría de sus datos en cero."
   ],
   "id": "94984b033a839ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analizamos nuestra variable objetivo, la que queremos predecir para saber si el dataset esta balanceado o no.\n",
    "Primero convertimos las variables categóricas en numéricas"
   ],
   "id": "b5de8ec89276adcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:18.084074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train['RainToday'] = train['RainToday'].map({'No': 0, 'Yes': 1})\n",
    "train['RainTomorrow'] = train['RainTomorrow'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "test['RainToday'] = test['RainToday'].map({'No': 0, 'Yes': 1})\n",
    "test['RainTomorrow'] = test['RainTomorrow'].map({'No': 0, 'Yes': 1})"
   ],
   "id": "4c6632d275a577d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:18.095101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculamos los porcentajes para el conjunto de entrenamiento\n",
    "train_percentages = train['RainTomorrow'].value_counts(normalize=True) * 100\n",
    "# Calculamos los porcentajes para el conjunto de prueba\n",
    "test_percentages = test['RainTomorrow'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='RainTomorrow', data=train)\n",
    "plt.title('Distribución de RainTomorrow - Train')\n",
    "for i, value in enumerate(train_percentages):\n",
    "    plt.text(i, train['RainTomorrow'].value_counts()[i], f'{value:.2f}%', ha='center')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='RainTomorrow', data=test)\n",
    "plt.title('Distribución de RainTomorrow - Test')\n",
    "for i, value in enumerate(test_percentages):\n",
    "    plt.text(i, test['RainTomorrow'].value_counts()[i], f'{value:.2f}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "a83da0fc5f7fcd03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Se observa que tanto en el conjuntos de entrenamiento como en el de prueba hay una mayor cantidad de datos donde no llueve (0) comparado con los días que llueve (1). Esto puede hacer que el modelo tenga un sesgo muy importante al momento de predecir. Por esta razón, podemos decir que los datos no se encuentran balanceados.\n",
    "### Podemos ver una relacion 3 a 1 aproximadamente, en este caso existe la posibilidad de balancear el dataset."
   ],
   "id": "6bb4386b9d5025db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Matriz de correlación ",
   "id": "2bb600e7cf71a161"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:18.470925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numeric_columns = numeric_columns.append(pd.Index(['RainTomorrow']))\n",
    "correlation_matrix_numeric = train[numeric_columns].corr()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix_numeric, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()"
   ],
   "id": "6d5377041cc75170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Podemos observar que las variables con mayor correlación positiva con RainfallTomorrow son:\n",
    "\n",
    "*   Rainfall (0.25)\n",
    "*   HumidityVariation (0.22)\n",
    "*   WindGustSpeed (0.17)\n",
    "\n",
    "Las variables con mayor correlación negativa con RainfallTomorrow son:\n",
    "*   Sunshine (-0.28)\n",
    "*   TempVariation (-0.23)\n",
    "\n",
    "Las variables con mayor correlación positiva con RainTomorrow son:\n",
    "\n",
    "*   RainfallTomorrow (0.55)\n",
    "*   HumidityVariation (0,27)\n",
    "*   Rainfall (0.24)\n",
    "*   WindGustSpeed (0,24)\n",
    "\n",
    "Las variables con mayor correlación negativa con RainTomorrow son:\n",
    "*   Sunshine (-0.37)\n",
    "*   TempVariation (-0.32)"
   ],
   "id": "e85a94310a8e3ae0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Para poder ver las metricas de test necesitamos aplicarle los mismos cambios que se hicieron en train.",
   "id": "f13ead3cd19c6731"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:19.061505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test.sort_values(by='Date', inplace=True)\n",
    "\n",
    "for column in test.columns:\n",
    "    test[column] = test[column].ffill()\n",
    "    test[column] = test[column].bfill() \n",
    "    \n",
    "columns_to_aggregate = ['Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm', 'Humidity9am', \n",
    "                        'Humidity3pm', 'Cloud9am', 'Cloud3pm','WindSpeed3pm','WindSpeed9am']\n",
    "new_columns = []\n",
    "test = test.drop(columns=['Unnamed: 0', 'Location','Date'])\n",
    "test['PressureVariation'] = test['Pressure3pm'] - test['Pressure9am']\n",
    "test['TempVariation'] = test['Temp3pm'] - test['Temp9am']\n",
    "test['HumidityVariation'] = test['Humidity3pm'] - test['Humidity9am']\n",
    "test['CloudVariation'] = test['Cloud3pm'] - test['Cloud9am']\n",
    "test['WindSpeedVariation'] = test['WindSpeed3pm'] - test['WindSpeed9am']\n",
    "test.drop(columns=columns_to_aggregate, inplace=True)\n"
   ],
   "id": "37f31e1aafeec8e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Método LinearRegression\n",
    "\n",
    "Evaluamos el modelo con los datos de entrenamiento y luego con los de test"
   ],
   "id": "722e344c8867de40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:19.089779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = train.drop(['RainfallTomorrow','WindGustDir','WindDir9am','WindDir3pm','RainTomorrow'], axis=1)\n",
    "y_train = train['RainfallTomorrow']\n",
    "X_test = test.drop(['RainfallTomorrow','WindGustDir','WindDir9am','WindDir3pm','RainTomorrow'], axis=1)\n",
    "y_test = test['RainfallTomorrow']\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test, \"Métricas sobre los datos de prueba:\")"
   ],
   "id": "8a6d40b0f4306541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Crear un explainer SHAP\n",
   "id": "2f3a77de87f78b4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:19.113841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "# Calcular los valores SHAP para el conjunto de prueba\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Gráfica de Resumen Global\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Gráfico de Dependencia Local (para la primera característica)\n",
    "shap.dependence_plot(\"FeatureName\", shap_values, X_test)\n",
    "\n",
    "# Gráfico de Dependencia Local (para la segunda característica)\n",
    "shap.dependence_plot(\"FeatureName\", shap_values, X_test)\n",
    "\n",
    "plt.show()"
   ],
   "id": "5d23b5c56f6d0be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### podemos ver que la varible mas importante es Rainfall y l variable menos importante MaxTemp. Con esto podemos decir que la Variable Rainfall es una de la mas importante par entender como funciona el modelo",
   "id": "6f3dc05554ab7a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### No se observa presencia de overfiting, podemos notar que las metricas de train y test son bastante similares",
   "id": "91f29ba192510fcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Método de gradiente descendiente",
   "id": "be5cd15f9e8c972b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.156984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sgd_model = SGDRegressor(loss='squared_error', max_iter=3000, random_state=42)\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred_train = sgd_model.predict(X_train)\n",
    "y_pred_test = sgd_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test, \"Métricas sobre los datos de prueba:\")"
   ],
   "id": "d1c1fd4b65c0d35e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notamos en este método, que las métricas de r2 comparadas con la regresión lineal son peores.",
   "id": "6f0a95823fd827c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Métodos de regularización Lasso",
   "id": "8f924640e2433560"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.158184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear y entrenar el modelo Lasso\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha es el parámetro de regularización\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_train = lasso_model.predict(X_train)\n",
    "y_pred_test = lasso_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "80700d4061ff9253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lasso con validación cruzada",
   "id": "bd98f259a0ff6b47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.160041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "lasso_cv_model = LassoCV(cv=5)\n",
    "lasso_cv_model.fit(X_train, y_train)\n",
    "y_pred_train_cv = lasso_cv_model.predict(X_train)\n",
    "y_pred_test_cv = lasso_cv_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_cv, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_cv, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "517be3a58655fd2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lasso con random_search",
   "id": "e65ea91c7dd53abe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.161709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define los rangos de valores para los hiperparámetros que quieres explorar\n",
    "param_distributions = {\n",
    "    'alpha': uniform(0.1, 100.0),  # Rango uniforme entre 0.1 y 100.0 para alpha\n",
    "}\n",
    "# Crea una instancia del modelo Lasso\n",
    "lasso_model = Lasso()\n",
    "# Crea una instancia de RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=lasso_model, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=10,  # Número de iteraciones de búsqueda aleatoria\n",
    "                                   scoring='neg_mean_squared_error',  # Métrica de evaluación\n",
    "                                   cv=5,  # Número de divisiones de validación cruzada\n",
    "                                   random_state=42)\n",
    "\n",
    "# Entrena el modelo usando RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Muestra los mejores parámetros encontrados\n",
    "print(f\"Mejores hiperparámetros encontrados:{random_search.best_params_}\" )\n",
    "# Hacer predicciones con el mejor modelo encontrado\n",
    "y_pred_train_rs = random_search.predict(X_train)\n",
    "y_pred_test_rs = random_search.predict(X_test)\n",
    "\n",
    "show_metrics_regresion(y_train, y_pred_train_rs, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_rs, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "ed6bfcf0371bde17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lasso con grid_search",
   "id": "a34c6c2ede0a313f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.162954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define los valores de los hiperparámetros que quieres explorar\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]  # Lista de valores para alpha\n",
    "}\n",
    "lasso_model = Lasso()\n",
    "grid_search = GridSearchCV(estimator=lasso_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Métrica de evaluación\n",
    "                           cv=5)  # Número de divisiones de validación cruzada\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Mejores hiperparámetros encontrados:{}\".format(grid_search.best_params_))\n",
    "y_pred_train_gs = grid_search.predict(X_train)\n",
    "y_pred_test_gs = grid_search.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_gs, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_gs, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "2fa0b8b57ead9fa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regresión de Ridge",
   "id": "f897c0f16e3b8099"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.164915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ridge_model = Ridge(alpha=0.1)  # alpha es el parámetro de regularización\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_train = ridge_model.predict(X_train)\n",
    "y_pred_test = ridge_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "842aa36fe521c58a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ridge con validación cruzada",
   "id": "3f4189afb501a1c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.166273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas = [0.001, 0.1, 0.3, 0.5, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]  # Puedes ajustar esta lista según sea necesario\n",
    "ridge_cv_model = RidgeCV(alphas=alphas)\n",
    "ridge_cv_model.fit(X_train, y_train)\n",
    "y_pred_train_cv = ridge_cv_model.predict(X_train)\n",
    "y_pred_test_cv = ridge_cv_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_cv, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_cv, \"Métricas sobre los datos de prueba:\")"
   ],
   "id": "63d223ad45e27b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ridge con random_search",
   "id": "1f6c137298cd795b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.167582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "param_distributions = {\n",
    "    'alpha': uniform(0.1, 100.0),  # Rango uniforme entre 0.1 y 100.0 para alpha\n",
    "}\n",
    "ridge_model = Ridge()\n",
    "random_search = RandomizedSearchCV(estimator=ridge_model, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=10,  # Número de iteraciones de búsqueda aleatoria\n",
    "                                   scoring='neg_mean_squared_error',  # Métrica de evaluación\n",
    "                                   cv=5,  # Número de divisiones de validación cruzada\n",
    "                                   random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Mejores hiperparámetros encontrados:{random_search.best_params_}\")\n",
    "y_pred_train_rs = random_search.predict(X_train)\n",
    "y_pred_test_rs = random_search.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_rs, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_rs, \"Métricas sobre los datos de prueba:\")"
   ],
   "id": "ae94c9fd96ed469b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ridge con grid_search",
   "id": "c9df99d7a369e71a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.169059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]  # Lista de valores para alpha\n",
    "}\n",
    "ridge_model = Ridge()\n",
    "grid_search = GridSearchCV(estimator=ridge_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Métrica de evaluación\n",
    "                           cv=5)  # Número de divisiones de validación cruzada\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Mejores hiperparámetros encontrados:{grid_search.best_params_}\")\n",
    "y_pred_train_gs = grid_search.predict(X_train)\n",
    "y_pred_test_gs = grid_search.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_gs, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_gs, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "f3a4b9ce65d226f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Método de regularización Elasticnet",
   "id": "91186741d6a6bb2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.170198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # alpha es el parámetro de regularización, l1_ratio controla la proporción de L1 y L2\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "y_pred_train = elastic_net_model.predict(X_train)\n",
    "y_pred_test = elastic_net_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test, \"Métricas sobre los datos de prueba:\")"
   ],
   "id": "76bcb052d5902628",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Elasticnet con validación cruzada",
   "id": "83d2fb33a35635fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.171859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas = [0.001, 0.1, 0.3, 0.5, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "l1_ratios = [0.1, 0.5, 0.9]\n",
    "elastic_net_cv_model = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios)\n",
    "elastic_net_cv_model.fit(X_train, y_train)\n",
    "y_pred_train_cv = elastic_net_cv_model.predict(X_train)\n",
    "y_pred_test_cv = elastic_net_cv_model.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_cv, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_cv, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "1888fd91fc4433cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Elasticnet con random_search",
   "id": "fa38827ef3e2d7a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.173114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_distributions = {\n",
    "    'alpha': uniform(0.01, 10.0),  # Rango uniforme entre 0.1 y 10.0 para alpha\n",
    "    'l1_ratio': uniform(0, 1),     # Rango uniforme entre 0 y 1 para l1_ratio\n",
    "}\n",
    "elastic_net_model = ElasticNet()\n",
    "random_search = RandomizedSearchCV(estimator=elastic_net_model, \n",
    "                                   param_distributions=param_distributions, \n",
    "                                   n_iter=100,  # Número de iteraciones de búsqueda aleatoria\n",
    "                                   scoring='neg_mean_squared_error',  # Métrica de evaluación\n",
    "                                   cv=5,  # Número de divisiones de validación cruzada\n",
    "                                   random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Mejores hiperparámetros encontrados:{random_search.best_params_}\")\n",
    "y_pred_train_rs = random_search.predict(X_train)\n",
    "y_pred_test_rs = random_search.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_rs, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_rs, \"Métricas sobre los datos de prueba:\")\n"
   ],
   "id": "e68e8c185a8166ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Elasticnet con grid_search",
   "id": "65129ec24864b5a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.174423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0],  # Lista de valores para alpha\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]  # Lista de valores para l1_ratio\n",
    "}\n",
    "elastic_net_model = ElasticNet()\n",
    "grid_search = GridSearchCV(estimator=elastic_net_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error',  # Métrica de evaluación\n",
    "                           cv=5)  # Número de divisiones de validación cruzada\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Mejores hiperparámetros encontrados:{grid_search.best_params_}\")\n",
    "y_pred_train_gs = grid_search.predict(X_train)\n",
    "y_pred_test_gs = grid_search.predict(X_test)\n",
    "show_metrics_regresion(y_train, y_pred_train_gs, \"Métricas sobre los datos de entrenamiento:\")\n",
    "show_metrics_regresion(y_test, y_pred_test_gs, \"Métricas sobre los datos de prueba:\")"
   ],
   "id": "2fd48badf01159d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Con respecto a las métricas, elegimos el r2 para comparar los distintos modelos\n",
    "### Necesitamos comparar los resultados obtenidos tanto en entrenamiento como en prueba para poder determinar si nuestro modelo esta ajustando correctamente."
   ],
   "id": "43c3f825078f96f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Regresión Logística"
   ],
   "id": "e15cee09ae0ec977"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Definimos las variables para clasificación",
   "id": "8a00f49c08b167bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.176169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train_clasification =train['RainTomorrow']\n",
    "y_test_clasification = test['RainTomorrow']"
   ],
   "id": "f87581704bf8d9b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.177397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_model1 = LogisticRegression(random_state=42)\n",
    "logistic_model1.fit(X_train, y_train_clasification)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_train = logistic_model1.predict(X_train)\n",
    "y_pred_test = logistic_model1.predict(X_test)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de entrenamiento\n",
    "ax1.text(0, 1, \"Classification Report para el conjunto de entrenamiento:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0.55, classification_report(y_train_clasification, y_pred_train), fontsize=10, family='monospace')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Añadir una pequeña separación vertical\n",
    "ax1.text(0, 0.5, \"-\" * 95, fontsize=10)\n",
    "\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de prueba\n",
    "ax1.text(0, 0.45, \"Classification Report para el conjunto de prueba:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0, classification_report(y_test_clasification, y_pred_test), fontsize=10, family='monospace')\n",
    "\n",
    "ax1.axis('off')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_clasification, y_pred_test)\n",
    "# Mostrar la matriz de confusión como un mapa de calor\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False, ax=ax2)\n",
    "ax2.set_xlabel('Predicción')\n",
    "ax2.set_ylabel('Valor Real')\n",
    "ax2.set_title('Matriz de Confusión')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "14c8e7aa9146945f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Crear un explainer SHAP para el modelo de clasificacion",
   "id": "f6b424ff55638721"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.178629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "explainer = shap.Explainer(logistic_model1, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "shap.dependence_plot(\"FeatureName\", shap_values, X_test)\n",
    "shap.dependence_plot(\"FeatureName\", shap_values, X_test)\n",
    "plt.show()"
   ],
   "id": "ddd004e77f85eeee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.179664Z"
    }
   },
   "cell_type": "code",
   "source": "### podemos observar que las variables que explicar mejor al modelo en este caso son WindGustSpeed , Rainfall. Por otro ldo podemos observar que la variable que menos los explica es CloudVariation",
   "id": "f30e21d6acf836cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regresión logística con penalización por clases",
   "id": "36061b790c0b534b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.180820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_model2 = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "logistic_model2.fit(X_train, y_train_clasification)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_train = logistic_model2.predict(X_train)\n",
    "y_pred_test = logistic_model2.predict(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de entrenamiento\n",
    "ax1.text(0, 1, \"Classification Report para el conjunto de entrenamiento:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0.55, classification_report(y_train_clasification, y_pred_train), fontsize=10, family='monospace')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Añadir una pequeña separación vertical\n",
    "ax1.text(0, 0.5, \"-\" * 95, fontsize=10)\n",
    "\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de prueba\n",
    "ax1.text(0, 0.45, \"Classification Report para el conjunto de prueba:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0, classification_report(y_test_clasification, y_pred_test), fontsize=10, family='monospace')\n",
    "\n",
    "ax1.axis('off')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_clasification, y_pred_test)\n",
    "# Mostrar la matriz de confusión como un mapa de calor\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False, ax=ax2)\n",
    "ax2.set_xlabel('Predicción')\n",
    "ax2.set_ylabel('Valor Real')\n",
    "ax2.set_title('Matriz de Confusión')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "72e628f69c797adf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Regresión logística con balanceo de clases SMOTE",
   "id": "bee63619ef307cf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.182122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_clasification)"
   ],
   "id": "1063615ca0dafdbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.183225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logistic_model3 = LogisticRegression(random_state=42)\n",
    "logistic_model3.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred_train = logistic_model3.predict(X_train)\n",
    "y_pred_test = logistic_model3.predict(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de entrenamiento\n",
    "ax1.text(0, 1, \"Classification Report para el conjunto de entrenamiento:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0.55, classification_report(y_train_clasification, y_pred_train), fontsize=10, family='monospace')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Añadir una pequeña separación vertical\n",
    "ax1.text(0, 0.5, \"-\" * 95, fontsize=10)\n",
    "\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de prueba\n",
    "ax1.text(0, 0.45, \"Classification Report para el conjunto de prueba:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0, classification_report(y_test_clasification, y_pred_test), fontsize=10, family='monospace')\n",
    "\n",
    "ax1.axis('off')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_clasification, y_pred_test)\n",
    "# Mostrar la matriz de confusión como un mapa de calor\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False, ax=ax2)\n",
    "ax2.set_xlabel('Predicción')\n",
    "ax2.set_ylabel('Valor Real')\n",
    "ax2.set_title('Matriz de Confusión')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "14a4e5cd93ec69cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.184589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtener las probabilidades de predicción de la clase positiva para entrenamiento y prueba para cada modelo\n",
    "y_pred_proba_train1 = logistic_model1.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_test1 = logistic_model1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_proba_train2 = logistic_model2.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_test2 = logistic_model2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_proba_train3 = logistic_model3.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_test3 = logistic_model3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calcular la curva ROC para entrenamiento y prueba para cada modelo\n",
    "fpr_train1, tpr_train1, _ = roc_curve(y_train_clasification, y_pred_proba_train1)\n",
    "fpr_test1, tpr_test1, _ = roc_curve(y_test_clasification, y_pred_proba_test1)\n",
    "auc_train1 = roc_auc_score(y_train_clasification, y_pred_proba_train1)\n",
    "auc_test1 = roc_auc_score(y_test_clasification, y_pred_proba_test1)\n",
    "\n",
    "fpr_train2, tpr_train2, _ = roc_curve(y_train_clasification, y_pred_proba_train2)\n",
    "fpr_test2, tpr_test2, _ = roc_curve(y_test_clasification, y_pred_proba_test2)\n",
    "auc_train2 = roc_auc_score(y_train_clasification, y_pred_proba_train2)\n",
    "auc_test2 = roc_auc_score(y_test_clasification, y_pred_proba_test2)\n",
    "\n",
    "fpr_train3, tpr_train3, _ = roc_curve(y_train_clasification, y_pred_proba_train3)\n",
    "fpr_test3, tpr_test3, _ = roc_curve(y_test_clasification, y_pred_proba_test3)\n",
    "auc_train3 = roc_auc_score(y_train_clasification, y_pred_proba_train3)\n",
    "auc_test3 = roc_auc_score(y_test_clasification, y_pred_proba_test3)\n",
    "\n",
    "# Trazar las curvas ROC para entrenamiento y prueba para cada modelo en una sola fila\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Modelo 1\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(fpr_train1, tpr_train1, color='blue', lw=2, label='Entrenamiento (AUC = %0.2f)' % auc_train1)\n",
    "plt.plot(fpr_test1, tpr_test1, color='red', lw=2, label='Prueba (AUC = %0.2f)' % auc_test1)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de falsos positivos')\n",
    "plt.ylabel('Tasa de verdaderos positivos')\n",
    "plt.title('Curva ROC - Regresión Logística')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Modelo 2\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(fpr_train2, tpr_train2, color='blue', lw=2, label='Entrenamiento (AUC = %0.2f)' % auc_train2)\n",
    "plt.plot(fpr_test2, tpr_test2, color='red', lw=2, label='Prueba (AUC = %0.2f)' % auc_test2)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de falsos positivos')\n",
    "plt.ylabel('Tasa de verdaderos positivos')\n",
    "plt.title('Curva ROC - Regresión Logística con penalización por clases')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Modelo 3\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(fpr_train3, tpr_train3, color='blue', lw=2, label='Entrenamiento (AUC = %0.2f)' % auc_train3)\n",
    "plt.plot(fpr_test3, tpr_test3, color='red', lw=2, label='Prueba (AUC = %0.2f)' % auc_test3)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de falsos positivos')\n",
    "plt.ylabel('Tasa de verdaderos positivos')\n",
    "plt.title('Curva ROC - Regresión Logística con balanceo de clases SMOTE')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Ajustar espaciado entre subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()"
   ],
   "id": "783f7890b36abebf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Podemos notar que al aplicar el método de sobre muestreo SMOTE o la regresión logística con penalización a la clase minoritaria podemos notar una mejora en los valores de recall",
   "id": "52f953700abfa18d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelo Clasificar base ",
   "id": "fe1aeb5df1e99853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.185532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear el clasificador ingenuo que predice la clase mayoritaria\n",
    "naive_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "naive_classifier.fit(X_train, y_train_clasification)\n",
    "\n",
    "# Hacer predicciones sobre los datos de entrenamiento y prueba\n",
    "y_pred_train = naive_classifier.predict(X_train)\n",
    "y_pred_test = naive_classifier.predict(X_test)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de entrenamiento\n",
    "ax1.text(0, 1, \"Classification Report para el conjunto de entrenamiento:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0.55, classification_report(y_train_clasification, y_pred_train), fontsize=10, family='monospace')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Añadir una pequeña separación vertical\n",
    "ax1.text(0, 0.5, \"-\" * 95, fontsize=10)\n",
    "\n",
    "# Mostrar el cuadro de texto (classification report) para el conjunto de prueba\n",
    "ax1.text(0, 0.45, \"Classification Report para el conjunto de prueba:\", fontsize=12, weight='bold')\n",
    "ax1.text(0, 0, classification_report(y_test_clasification, y_pred_test), fontsize=10, family='monospace')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_clasification, y_pred_test)\n",
    "# Mostrar la matriz de confusión como un mapa de calor\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False, ax=ax2)\n",
    "ax2.set_xlabel('Predicción')\n",
    "ax2.set_ylabel('Valor Real')\n",
    "ax2.set_title('Matriz de Confusión')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "da1f443f8d4947c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## curva ROC de modelo base",
   "id": "28ca437b1565060f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.186620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtener las probabilidades de predicción de la clase positiva (la clase mayoritaria)\n",
    "y_pred_proba_train = np.full_like(y_train_clasification, fill_value=1)  # Clase mayoritaria\n",
    "y_pred_proba_test = np.full_like(y_test_clasification, fill_value=1)  # Clase mayoritaria\n",
    "\n",
    "# Calcular la curva ROC para los datos de entrenamiento y prueba\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train_clasification, y_pred_proba_train)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_clasification, y_pred_proba_test)\n",
    "\n",
    "# Calcular el área bajo la curva ROC (AUC) para los datos de entrenamiento y prueba\n",
    "auc_train = roc_auc_score(y_train_clasification, y_pred_proba_train)\n",
    "auc_test = roc_auc_score(y_test_clasification, y_pred_proba_test)\n",
    "\n",
    "# Crear la figura con dos subplots uno al lado del otro\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Trazar la curva ROC para los datos de entrenamiento\n",
    "ax1.plot(fpr_train, tpr_train, color='blue', lw=2, label='Train ROC curve (AUC = %0.2f)' % auc_train)\n",
    "ax1.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('Tasa de falsos positivos')\n",
    "ax1.set_ylabel('Tasa de verdaderos positivos')\n",
    "ax1.set_title('Curva ROC para datos de entrenamiento')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Trazar la curva ROC para los datos de prueba\n",
    "ax2.plot(fpr_test, tpr_test, color='red', lw=2, label='Test ROC curve (AUC = %0.2f)' % auc_test)\n",
    "ax2.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('Tasa de falsos positivos')\n",
    "ax2.set_ylabel('Tasa de verdaderos positivos')\n",
    "ax2.set_title('Curva ROC para datos de prueba')\n",
    "ax2.legend(loc='lower right')\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.show()\n"
   ],
   "id": "a3f61d2b5e6cb3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modelo Regresion base ",
   "id": "6cb9ea938a6082da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.187856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear un DummyRegressor que predice la media de la variable objetivo\n",
    "dummy_regressor = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Entrenar el modelo de regresión base con los datos de entrenamiento\n",
    "dummy_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones con el modelo de regresión base en los datos de entrenamiento y prueba\n",
    "y_pred_train_base = dummy_regressor.predict(X_train)\n",
    "y_pred_test_base = dummy_regressor.predict(X_test)\n",
    "\n",
    "# Calcular el error cuadrático medio (MSE) del modelo de regresión base para el conjunto de entrenamiento y prueba\n",
    "mse_train_base = mean_squared_error(y_train, y_pred_train_base)\n",
    "mse_test_base = mean_squared_error(y_test, y_pred_test_base)\n",
    "\n",
    "# Calcular el Error Absoluto Medio (MAE) para el conjunto de entrenamiento y prueba\n",
    "mae_train_base = mean_absolute_error(y_train, y_pred_train_base)\n",
    "mae_test_base = mean_absolute_error(y_test, y_pred_test_base)\n",
    "\n",
    "# Calcular el Error Porcentual Absoluto Medio (MAPE) para el conjunto de prueba\n",
    "mape_test_base = np.mean(np.abs((y_test - y_pred_test_base) / y_test)) * 100\n",
    "\n",
    "# Calcular el coeficiente de determinación (R-cuadrado) para el conjunto de prueba\n",
    "r2_test_base = r2_score(y_test, y_pred_test_base)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(\"Métricas para el modelo de regresión base:\")\n",
    "print(\"Error cuadrático medio (MSE) (Train):\", mse_train_base)\n",
    "print(\"Error cuadrático medio (MSE) (Test):\", mse_test_base)\n",
    "print(\"Mean Absolute Error (MAE) (Train):\", mae_train_base)\n",
    "print(\"Mean Absolute Error (MAE) (Test):\", mae_test_base)\n",
    "print(\"Mean Absolute Percentage Error (MAPE) (Test):\", mape_test_base)\n",
    "print(\"R-squared (Test):\", r2_test_base)\n"
   ],
   "id": "a7d7945def41f2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Red neuronal para Regresion",
   "id": "e92b5dec5ebb008a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.188964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(12,), activation='relu'))\n",
    "model.add(Dense(1, activation='linear')) \n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "show_metrics_regresion(y_train, y_pred_train,\"Métricas del conjunto de entrenamiento:\", False)\n",
    "show_metrics_regresion(y_test, y_pred_test,\"Métricas del conjunto de Prueba:\", False)"
   ],
   "id": "16907cf6e855d601",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Optuna",
   "id": "9069ba9aee0dfa42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T19:54:20.261295Z",
     "start_time": "2024-05-02T19:54:20.190136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)\n",
    "    hidden_layer_size = trial.suggest_int('hidden_layer_size', 16, 64)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, input_shape=(12,), activation='relu'))\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_layer_size, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Definir EarlyStopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Entrenar el modelo con EarlyStopping\n",
    "    history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    return mse_train\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_model = Sequential()\n",
    "best_model.add(Dense(best_params['hidden_layer_size'], input_shape=(12,), activation='relu'))\n",
    "for _ in range(best_params['num_hidden_layers']):\n",
    "    best_model.add(Dense(best_params['hidden_layer_size'], activation='relu'))\n",
    "best_model.add(Dense(1, activation='linear'))\n",
    "best_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "best_model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "print(\"Mejores Hiperparámetros:\", best_params)\n",
    "show_metrics_regresion(y_test, y_pred_test,\"conjunto de prueba:\",nr_neuronal=False)\n"
   ],
   "id": "8cb731837c79450b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Red neuronal para Clasificacion\n",
   "id": "52090374f3fefa82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.191388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Construir el modelo de red neuronal\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Capa de salida con activación sigmoide para clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train_clasification, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_clasification, verbose=0)\n",
    "print(\"Precisión en el conjunto de entrenamiento:\", train_accuracy)\n",
    "print(\"Pérdida en el conjunto de entrenamiento:\", train_loss)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Precisión en el conjunto de prueba:\", test_accuracy)\n",
    "print(\"Pérdida en el conjunto de prueba:\", test_loss)\n",
    "\n",
    "\n"
   ],
   "id": "aaea2b188bfdf9c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## optimizacion de hiper-parametros",
   "id": "81c62550b9c0b0d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.192564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_accuracy = 0.0  # Seguimiento de la mejor precisión encontrada\n",
    "def objective(trial):\n",
    "    global best_accuracy  # Para acceder a la variable global\n",
    "\n",
    "    # Definir los hiperparámetros a optimizar\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 3)\n",
    "    hidden_layer_size = trial.suggest_int('hidden_layer_size', 16, 64)\n",
    "\n",
    "    # Construir el modelo de red neuronal\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layer_size, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(hidden_layer_size, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Capa de salida con activación sigmoide para clasificación binaria\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train_clasification, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    _, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Si es el mejor modelo hasta ahora, actualizar la mejor precisión encontrada\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "# Construir el mejor modelo con los mejores hiperparámetros\n",
    "best_model = Sequential()\n",
    "best_model.add(Dense(best_params['hidden_layer_size'], input_shape=(X_train.shape[1],), activation='relu'))\n",
    "for _ in range(best_params['num_hidden_layers']):\n",
    "    best_model.add(Dense(best_params['hidden_layer_size'], activation='relu'))\n",
    "best_model.add(Dense(1, activation='sigmoid'))  # Capa de salida con activación sigmoide para clasificación binaria\n",
    "\n",
    "best_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "best_model.fit(X_train, y_train_clasification, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluar el mejor modelo en el conjunto de entrenamiento y de prueba\n",
    "train_loss, train_accuracy = best_model.evaluate(X_train, y_train_clasification, verbose=0)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"\\nMétricas del mejor modelo:\")\n",
    "print(\"Precisión en el conjunto de entrenamiento:\", train_accuracy)\n",
    "print(\"Pérdida en el conjunto de entrenamiento:\", train_loss)\n",
    "print(\"Precisión en el conjunto de prueba:\", test_accuracy)\n",
    "print(\"Pérdida en el conjunto de prueba:\", test_loss)\n"
   ],
   "id": "e651b51a4b8018e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### basado en las metricas obtenidas vamos a usar La regresion logistica para el problema de clasificacion, y para el problema de regresion la res neuronal",
   "id": "c58d4bb6d795e7d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-02T19:54:20.193814Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a3cbebb93fceb382",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
